```
.:
LICENSE
docker-compose.yml
dockerfile
output.txt
pyproject.toml
r.sh
src
test.sh
tests

./src:
simple_scraper

./src/simple_scraper:
__init__.py
browser.py
core.py
model.py
strategies
utils.py

./src/simple_scraper/strategies:
__init__.py
mobile.py
proxy.py
stealth.py

./tests:
__pycache__
conftest.py
site
test_intergration_site.py
test_unit.py

./tests/__pycache__:
conftest.cpython-310-pytest-8.4.1.pyc
test_intergration_site.cpython-310-pytest-8.4.1.pyc
test_unit.cpython-310-pytest-8.4.1.pyc

./tests/site:
attrs_mixed.html
dynamic.html
dynamic_href.html
dynamic_insert.html
empty.html
encoding_utf8.html
index.html
links.html
longlist.html
nested.html
visibility.html
```

```
./.github/workflows/release.yml

name: Release

on:
  push:
    tags:
      - "v*.*.*"
  workflow_dispatch: {}

permissions:
  contents: read
  id-token: write

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    environment: pypi

    env:
      # headless_shell 강제 사용 방지
      PW_CHROMIUM_USE_HEADLESS_SHELL: "0"
      PLAYWRIGHT_CHROMIUM_USE_HEADLESS_SHELL: "0"
      # (옵션) 채널 강제하고 싶으면 chrome 지정, 기본은 빈 값
      PW_CHANNEL: ""

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Upgrade pip and install build tools
        shell: bash
        run: |
          python -m pip install -U pip
          python -m pip install build

      - name: Install package (dev)
        shell: bash
        run: |
          python -m pip install ".[dev]"

      # 브라우저와 OS deps를 한 번에 설치 (chromium 기본 + chrome 채널도 함께 설치해둠)
      - name: Install Playwright browsers (with OS deps)
        shell: bash
        run: |
          python -m pip install playwright
          python -m playwright install --with-deps chromium chrome
          python -m playwright --version
          ls -R /home/runner/.cache/ms-playwright/ || true

      - name: Run tests
        shell: bash
        run: pytest -q

      - name: Build sdist and wheel
        shell: bash
        run: python -m build

      - name: Publish to PyPI via OIDC
        if: startsWith(github.ref, 'refs/tags/')
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          skip-existing: true

```

```
./pyproject.toml

[project]
name = "simple-scraper"
dynamic = ["version"]
description = "A simple and light Playwright-based scraper"
authors = [
    { name = "elecbrandy", email = "elecbrandy@gmail.com" }
]
readme = "README.md"
license = { text = "MIT" }
requires-python = ">=3.9"
dependencies = [
    "playwright>=1.44",
    "nest_asyncio>=1.5.6"
]
keywords = ["playwright", "scraping", "web-scraping", "automation", "crawler"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Internet :: WWW/HTTP",
    "Topic :: Software Development :: Libraries :: Python Modules"
]

[project.optional-dependencies]
dev = [
    "pytest",
    "black",
    "isort"
]

[project.urls]
Homepage = "https://github.com/elecbrandy/simple-scraper"
Repository = "https://github.com/elecbrandy/simple-scraper"
Issues = "https://github.com/elecbrandy/simple-scraper/issues"

[build-system]
requires = ["setuptools>=61", "wheel", "setuptools-scm"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]
include = ["simple_scraper*"]
exclude = ["tests*", "examples*"]

[tool.pytest.ini_options]
addopts = "-q"
testpaths = ["tests"]
markers = [
  "e2e: end-to-end tests that launch a browser (slow)",
]
filterwarnings = [
  "ignore::DeprecationWarning",
]

[tool.setuptools_scm]
version_scheme = "guess-next-dev"
local_scheme = "no-local-version"
fallback_version = "0.0.0"
```

```
./src/simple_scraper/strategies/__init__.py


```

```
./src/simple_scraper/strategies/mobile.py

import asyncio
from typing import List, Optional
from playwright.async_api import Browser
from ..utils import realistic_headers, extract_elements

MOBILE_UA = "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1"

async def run(
        browser: Browser,
        url: str,
        selector: str,
        attribute: Optional[str],
        timeout: int,
    ) -> List[str]:

    # Set up the browser context for mobile emulation
    ctx = await browser.new_context(
        user_agent=MOBILE_UA,
        viewport={"width": 375, "height": 667},
        device_scale_factor=2,
        is_mobile=True,
        has_touch=True,
        extra_http_headers=realistic_headers(),
    )
    
    # Add a mobile-specific script to the context
    page = await ctx.new_page()
    await page.goto(url, wait_until="domcontentloaded", timeout=timeout)
    for _ in range(3):
        await page.evaluate("window.scrollBy(0, 200)")
        await asyncio.sleep(0.8)
    await page.wait_for_selector(selector, state="attached", timeout=timeout)

    result = await extract_elements(page, selector, attribute)
    await ctx.close()
    return result
```

```
./src/simple_scraper/strategies/proxy.py

import random
from typing import List, Optional
from playwright.async_api import Browser
from ..utils import UA_POOL, realistic_headers, extract_elements, simulate_human

async def run(
        browser: Browser,
        url: str,
        selector: str,
        attribute: Optional[str],
        timeout: int,
    ) -> List[str]:
        headers = {
            **realistic_headers(),
            "X-Forwarded-For": ".".join(str(random.randint(1, 255)) for _ in range(4)),
            "X-Real-IP": ".".join(str(random.randint(1, 255)) for _ in range(4)),
            "Via": "1.1 proxy-server",
        }
        ctx = await browser.new_context(
            user_agent=random.choice(UA_POOL),
            viewport={"width": 1440, "height": 900},
            extra_http_headers=headers,
        )

        page = await ctx.new_page()
        await page.goto(url, wait_until="load", timeout=timeout)
        await simulate_human(page)
        await page.wait_for_selector(selector, state="attached", timeout=timeout)
        
        result = await extract_elements(page, selector, attribute)
        await ctx.close()
        return result

```

```
./src/simple_scraper/strategies/stealth.py

import re
import random
from typing import List, Optional
from playwright.async_api import Browser
from ..utils import UA_POOL, STEALTH_SCRIPT, realistic_headers, simulate_human, extract_elements

async def run(
        browser: Browser,
        url: str,
        selector: str,
        attribute: Optional[str],
        timeout: int,
    ) -> List[str]:

    # Set up the browser context with stealth features
    ctx = await browser.new_context(
        user_agent=random.choice(UA_POOL),
        locale="en-US",
        extra_http_headers=realistic_headers(),
        viewport={"width": 1440, "height": 900},
        java_script_enabled=True,
    )

    # Add the stealth script to the context
    await ctx.add_init_script(STEALTH_SCRIPT)
    page = await ctx.new_page()
    _asset_re = re.compile(r".*\.(png|jpe?g|gif|svg|css|woff2?)$", re.IGNORECASE)
    await page.route(_asset_re, lambda r: r.abort())

    # Navigate to the URL and wait for the page to load
    await page.goto(url, wait_until="domcontentloaded", timeout=timeout)
    await simulate_human(page)
    await page.wait_for_selector(selector, state="attached", timeout=timeout)

    result = await extract_elements(page, selector, attribute)
    await ctx.close()
    return result

```

```
./src/simple_scraper/__init__.py

from .core import scrape_context, scrape_href
from .model import ScrapeResult

__all__ = ["scrape_context", "scrape_href", "ScrapeResult"]

```

```
./src/simple_scraper/browser.py

import os
from typing import List, Optional
from playwright.async_api import Browser, Playwright

async def launch_chromium(
		pw: Playwright,
		headless: bool = True,
		extra_args: Optional[List[str]] = None
	) -> Browser:
	is_ci = os.getenv("CI") or os.getenv("GITHUB_ACTIONS")
	channel = os.getenv("PW_CHANNEL") or None
	args = [
		"--no-sandbox",							# disable sandboxing
		"--disable-setuid-sandbox",				# disable setuid sandbox
		"--disable-dev-shm-usage",				# disable /dev/shm usage
		"--disable-blink-features=AutomationControlled", # disable automation features
		"--disable-features=TranslateUI",		# disable translation UI
		"--mute-audio",							# mute audio
		"--disable-gpu",						# disable GPU hardware acceleration
		"--disable-extensions",					# disable extensions
		"--disable-background-networking",		# disable background networking
		"--disable-sync",						# disable sync
		"--disable-default-apps",				# disable default apps
		"--no-first-run",						# disable first run UI
	]
	if extra_args:
		args += extra_args

	# Launch the browser (채널이 지정되면 Chrome 채널 사용)
	launch_kwargs = dict(headless=headless, args=args)
	if channel:
		launch_kwargs["channel"] = channel
	browser = await pw.chromium.launch(**launch_kwargs)
	return browser


```

```
./src/simple_scraper/core.py

import asyncio
import importlib
import random
from typing import List, Optional
from .model import ScrapeResult

from playwright.async_api import async_playwright, Browser
from .browser import launch_chromium

# strategies to try in order
_STRATEGY_PATHS = [
	"simple_scraper.strategies.stealth",
	"simple_scraper.strategies.mobile",
	"simple_scraper.strategies.proxy",
]

async def _run_strategies(
		url: str,
		selector: str,
		attribute: Optional[str],
		headless: bool,
		timeout: int,
	) -> List[str]:
	last_err: Optional[Exception] = None
	browser: Optional[Browser] = None

	# Start Playwright
	async with async_playwright() as pw:
		try:
			browser = await launch_chromium(pw, headless=headless, extra_args=[
				"--disable-features=TranslateUI", "--mute-audio"
			])
			# Try each strategy in order
			for i, mod_path in enumerate(_STRATEGY_PATHS):
				strat = importlib.import_module(mod_path)
				try:
					return await strat.run(
						browser, url, selector, attribute, timeout=timeout
					)
				except Exception as e:
					last_err = e
					await asyncio.sleep((i + 1) * 2 + random.uniform(1, 3))
		finally:
			# Ensure the browser is closed
			if browser:
				await browser.close()
	raise RuntimeError(f"All strategies failed: {last_err}")

def _run_sync(
		url: str,
		selector: str,
		attribute: Optional[str],
		headless: bool,
		timeout: int,
	) -> List[str]:
	try:
		# Check if we are already in an event loop
		loop = asyncio.get_running_loop()
		import nest_asyncio; nest_asyncio.apply()
		return loop.run_until_complete(_run_strategies(url, selector, attribute, headless, timeout))
	except RuntimeError:
		# If not, create a new event loop
		return asyncio.run(_run_strategies(url, selector, attribute, headless, timeout))

def _validate_inputs(url: str, selector: str) -> None:
	if not isinstance(url, str):
		raise TypeError("URL must be a string")
	if not url:
		raise ValueError("URL must not be empty")
	if not isinstance(selector, str):
		raise TypeError("Selector must be a string")
	if not selector:
		raise ValueError("Selector must not be empty")
	
def _respect_robots(url: str, respect_robots: bool, robots_user_agent: str) -> None:
	# will be developed in the future
	return

def scrape_context(
	url: str,
	selector: str,
	respect_robots: bool = True,
	user_agent: str = "*",
	headless: bool = True,
	timeout: int = 30,
	) -> ScrapeResult:
	"""Return the text of all elements that match a CSS selector.

	Uses Playwright to open the page, tries a few safe methods,
	and collects each element’s inner text (trimmed).

	Args:
		url: Page URL.
		selector: CSS selector to match.
		respect_robots: Whether to respect robots.txt. (Not implemented yet)
		user_agent: User agent to use for robots.txt check. (Not implemented yet)
		headless: Whether to run the browser in headless mode.
		timeout: Maximum time to wait for the page to load (in seconds).

	Returns:
		ScrapeResult
			* containing the texts, URL, and other metadata.
			* elements with no text content are skipped (only non-empty strings are returned).
	"""
	_validate_inputs(url, selector)
	# _respect_robots(url, respect_robots, user_agent) # Not implemented yet
	data = _run_sync(url, selector, None, headless=headless, timeout=timeout*1000)
	return ScrapeResult(url=url, selector=selector, result=data)


def scrape_href(
	url: str,
	selector: str,
	respect_robots: bool = True,
	user_agent: str = "*",
	headless: bool = True,
	timeout: int = 30,
	) -> ScrapeResult:
	"""Return the href attributes of all elements that match a CSS selector.

	Uses Playwright to open the page, tries a few safe methods,
	and collects each element’s href attribute.

	Args:
		url: Page URL.
		selector: CSS selector to match.
		respect_robots: Whether to respect robots.txt. (Not implemented yet)
		user_agent: User agent to use for robots.txt check. (Not implemented yet)
		headless: Whether to run the browser in headless mode.
		timeout: Maximum time to wait for the page to load (in seconds).

	Returns:
		ScrapeResult
			* containing the non-empty href strings, URL, and other metadata.
			* elements without an href (or with an empty href) are skipped and not included.
	"""
	_validate_inputs(url, selector)
	# _respect_robots(url, respect_robots, user_agent) # Not implemented yet
	data = _run_sync(url, selector, "href", headless=headless, timeout=timeout*1000)
	return ScrapeResult(url=url, selector=selector, result=data)

```

```
./src/simple_scraper/model.py

from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional

@dataclass
class ScrapeResult:
    """Data class to hold the result of a scrape operation."""
    url: str
    selector: str
    result: List[str]
    count: int = field(init=False)
    fetched_at: datetime = field(default_factory=datetime.utcnow)

    def __post_init__(self):
        self.count = len(self.result)

    def first(self) -> Optional[str]:
        return self.result[0] if self.result else None

    def to_dict(self) -> dict:
        return {
            "url": self.url,
            "selector": self.selector,
            "result": self.result,
            "count": self.count,
            "fetched_at": self.fetched_at.isoformat(),
        }

```

```
./src/simple_scraper/utils.py

import random
import asyncio
from typing import List, Optional
from urllib.parse import urlparse, urlunparse
from playwright.async_api import Page

UA_POOL: List[str] = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

STEALTH_SCRIPT: str = """
Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
Object.defineProperty(navigator, 'languages', { get: () => ['en-US','en'] });
window.chrome = { runtime: {}, loadTimes: () => {}, csi: () => {}, app: {} };
Object.defineProperty(navigator, 'permissions', { get: () => ({ query: () => Promise.resolve({state:'granted'}) })});
"""

def realistic_headers() -> dict:
    return {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

async def simulate_human(page: Page):
    for _ in range(random.randint(2, 5)):
        await page.mouse.move(random.randint(50, 800), random.randint(50, 600))
        await asyncio.sleep(random.uniform(0.1, 0.3))
    for _ in range(random.randint(1, 3)):
        await page.mouse.wheel(0, random.randint(150, 400))
        await asyncio.sleep(random.uniform(0.5, 1.2))
    await asyncio.sleep(random.uniform(0.3, 0.8)) 

async def extract_elements(page: Page, selector: str, attribute: Optional[str]) -> List[str]:
    locator = page.locator(selector)
    elements = await locator.all()
    out: List[str] = []
    for el in elements:
        if attribute:
            value = await el.get_attribute(attribute)
        else:
            value = await el.inner_text()
        if value and value.strip():
            out.append(value.strip())
    return out

```

```
./tests/conftest.py

import contextlib
import socket
import threading
from http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer
from pathlib import Path
import pytest

def _free_port():
    with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
        s.bind(("127.0.0.1", 0))
        return s.getsockname()[1]

@pytest.fixture(scope="session")
def site_root():
    """Returns the path to the site directory."""
    return Path(__file__).parent / "site"

@pytest.fixture(scope="session")
def http_server(site_root):
    """Starts a simple HTTP server serving the site directory."""
    class Handler(SimpleHTTPRequestHandler):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, directory=str(site_root), **kwargs)

    port = _free_port()
    server = ThreadingHTTPServer(("127.0.0.1", port), Handler)
    thread = threading.Thread(target=server.serve_forever, daemon=True)
    thread.start()
    try:
        yield f"http://127.0.0.1:{port}"
    finally:
        server.shutdown()
        thread.join()

```

```
./tests/site/attrs_mixed.html

<!doctype html><meta charset="utf-8"><title>Attrs Mixed</title>
<nav>
  <a class="nav" href="/ok.html">OK</a>
  <a class="nav">NO HREF</a>
  <a class="nav" href="">EMPTY</a>
  <a class="nav" href="   ">SPACE</a>
  <a class="nav" href="javascript:void(0)">JS</a>
</nav>

```

```
./tests/site/dynamic.html

<!doctype html><meta charset="utf-8"><title>Dynamic</title>
<div id="app"></div>
<script>
  setTimeout(() => {
    const el = document.createElement('div');
    el.id = 'late';
    el.textContent = 'I came late';
    document.getElementById('app').appendChild(el);
  }, 600);
</script>

```

```
./tests/site/dynamic_href.html

<!doctype html><meta charset="utf-8"><title>Dynamic Href</title>
<nav id="nav"></nav>
<script>
  setTimeout(()=>{
    const a = document.createElement('a');
    a.className = 'later';
    a.textContent = 'Go';
    a.href = '/a.html';    // 생성 시점에 href 이미 존재
    document.getElementById('nav').appendChild(a);
  }, 500);
</script>

```

```
./tests/site/dynamic_insert.html

<!doctype html><meta charset="utf-8"><title>Dynamic Insert</title>
<ul id="list"></ul>
<script>
  window.addEventListener('DOMContentLoaded', () => {
    setTimeout(() => {
      const ul = document.getElementById('list');
      ["One","Two","Three"].forEach(t => {
        const li = document.createElement('li');
        li.className = 'item';
        li.textContent = t;
        ul.appendChild(li);
      });
    }, 600);
  });
</script>

```

```
./tests/site/empty.html

<!doctype html><meta charset="utf-8">
<title>Empty</title>
<div class="has-text">  </div>
<div class="has-text">Not empty</div>
<a class="maybe-href"></a>

```

```
./tests/site/encoding_utf8.html

<!doctype html><meta charset="utf-8"><title>UTF-8</title>
<p class="ko">  안녕하세요&nbsp;세계  </p>
<p class="ko">탭\t과  공백   혼합</p>

```

```
./tests/site/index.html

<!doctype html><meta charset="utf-8">
<title>Index</title>
<h1>Example Heading</h1>
<ul>
  <li class="item">First</li>
  <li class="item">Second</li>
  <li class="item">Third</li>
</ul>

```

```
./tests/site/links.html

<!doctype html><meta charset="utf-8">
<title>Links</title>
<nav>
  <a class="nav" href="/a.html">Go A</a>
  <a class="nav" href="/b.html">Go B</a>
</nav>

```

```
./tests/site/longlist.html

<!doctype html><meta charset="utf-8"><title>Long List</title>
<ul id="big">
  <li class="row">Item 01</li><li class="row">Item 02</li><li class="row">Item 03</li>
  <li class="row">Item 04</li><li class="row">Item 05</li><li class="row">Item 06</li>
  <li class="row">Item 07</li><li class="row">Item 08</li><li class="row">Item 09</li>
  <li class="row">Item 10</li><li class="row">Item 11</li><li class="row">Item 12</li>
  <li class="row">Item 13</li><li class="row">Item 14</li><li class="row">Item 15</li>
  <li class="row">Item 16</li><li class="row">Item 17</li><li class="row">Item 18</li>
  <li class="row">Item 19</li><li class="row">Item 20</li><li class="row">Item 21</li>
  <li class="row">Item 22</li><li class="row">Item 23</li><li class="row">Item 24</li>
  <li class="row">Item 25</li><li class="row">Item 26</li><li class="row">Item 27</li>
  <li class="row">Item 28</li><li class="row">Item 29</li><li class="row">Item 30</li>
</ul>

```

```
./tests/site/nested.html

<!doctype html><meta charset="utf-8"><title>Nested</title>
<section id="news">
  <article class="card">
    <h2><a class="headline" href="/a.html">Alpha</a></h2>
    <p>First paragraph</p>
  </article>
  <article class="card">
    <h2><a class="headline" href="/b.html">Beta</a></h2>
    <p>Second paragraph</p>
  </article>
</section>
<ul id="tags">
  <li><span class="tag">AI</span></li>
  <li><span class="tag">Economy</span></li>
</ul>

```

```
./tests/site/visibility.html

<!doctype html><meta charset="utf-8"><title>Visibility</title>
<style>.hidden{display:none}</style>
<div class="msg hidden">I will appear</div>
<script>
  setTimeout(() => {
    document.querySelector('.msg').classList.remove('hidden');
  }, 500);
</script>

```

```
./tests/test_intergration_site.py

import pytest
from simple_scraper import scrape_context, scrape_href

# 이 파일 전체를 e2e로 표시 (브라우저 실행 필요)
pytestmark = pytest.mark.e2e

def _u(http_server: str, path: str) -> str:
    return f"{http_server}/{path}"

def test_index_list_extraction(http_server):
    res = scrape_context(_u(http_server, "index.html"), "li.item", headless=True, timeout=5)
    assert res.count == 3
    assert res.first() == "First"
    assert all(isinstance(x, str) and x for x in res.result)

def test_links_href_extraction(http_server):
    res = scrape_href(_u(http_server, "links.html"), "a.nav", headless=True, timeout=5)
    assert res.count == 2
    assert set(res.result) == {"/a.html", "/b.html"}

def test_dynamic_wait_visible_after_insert(http_server):
    # dynamic.html 은 일정 시간 후 #late 요소가 생성되어야 함
    res = scrape_context(_u(http_server, "dynamic.html"), "#late", headless=True, timeout=5)
    assert res.count == 1 and res.result[0] == "I came late"

def test_dynamic_list_insert(http_server):
    # dynamic_insert.html 은 li.item 이 지연 삽입됨
    res = scrape_context(_u(http_server, "dynamic_insert.html"), "li.item", headless=True, timeout=5)
    assert res.result == ["One", "Two", "Three"]

def test_dynamic_href_added_later(http_server):
    # 파일명이 dynamic_herf.html 임에 주의 (오타 그대로 사용)
    res = scrape_href(_u(http_server, "dynamic_href.html"), "a.later", headless=True, timeout=5)
    assert res.result == ["/a.html"]

def test_attrs_mixed_href_filter(http_server):
    # 빈 문자열/공백/누락된 href는 제외, javascript:void(0)은 포함
    res = scrape_href(_u(http_server, "attrs_mixed.html"), ".nav", headless=True, timeout=5)
    assert set(res.result) == {"/ok.html", "javascript:void(0)"}

def test_empty_text_is_filtered_and_empty_href_ignored(http_server):
    # 공백 텍스트는 필터링되어 "Not empty"만 남아야 함
    ctx = scrape_context(_u(http_server, "empty.html"), ".has-text", headless=True, timeout=5)
    assert ctx.result == ["Not empty"]
    # href 속성이 비어있는 경우 결과에서 제외
    hrefs = scrape_href(_u(http_server, "empty.html"), "a.maybe-href", headless=True, timeout=5)
    assert hrefs.result == []

def test_encoding_utf8_paragraphs(http_server):
    # 유니코드 텍스트가 잘 추출되는지 확인(정확 공백매칭 대신 부분 문자열 검사)
    res = scrape_context(_u(http_server, "encoding_utf8.html"), "p.ko", headless=True, timeout=5)
    assert res.count == 2
    assert ("안녕하세요" in res.result[0]) and ("세계" in res.result[0])
    assert ("탭" in res.result[1]) and ("혼합" in res.result[1])

def test_longlist_count_and_edges(http_server):
    res = scrape_context(_u(http_server, "longlist.html"), "li.row", headless=True, timeout=5)
    assert res.count == 30
    assert res.first() == "Item 01"
    assert res.result[-1] == "Item 30"

def test_nested_headlines_text_and_links(http_server):
    txt = scrape_context(_u(http_server, "nested.html"), ".card h2 .headline", headless=True, timeout=5)
    assert txt.result == ["Alpha", "Beta"]
    hrefs = scrape_href(_u(http_server, "nested.html"), ".card h2 .headline", headless=True, timeout=5)
    assert hrefs.result == ["/a.html", "/b.html"]

def test_visibility_toggle(http_server):
    res = scrape_context(_u(http_server, "visibility.html"), ".msg", headless=True, timeout=5)
    assert res.result == ["I will appear"]

def test_missing_selector_raises(http_server):
    with pytest.raises(RuntimeError):
        scrape_context(_u(http_server, "index.html"), ".does-not-exist", headless=True, timeout=2)

```

```
./tests/test_unit.py

import pytest
from simple_scraper.core import scrape_context
from simple_scraper.model import ScrapeResult
from simple_scraper import utils

def test_validation_errors():
    with pytest.raises(TypeError):
        scrape_context(123, "a")
    with pytest.raises(ValueError):
        scrape_context("", "a")
    with pytest.raises(TypeError):
        scrape_context("http://x", 123)
    with pytest.raises(ValueError):
        scrape_context("http://x", "")

def test_model_to_dict():
    s = ScrapeResult(url="u", selector="s", result=["a","b","c"])
    d = s.to_dict()
    assert d["count"] == 3 and d["url"] == "u" and d["selector"] == "s"

def test_utils_headers_and_uapool():
    headers = utils.realistic_headers()
    assert "Accept" in headers and "Accept-Language" in headers
    assert isinstance(utils.UA_POOL, list) and len(utils.UA_POOL) >= 3

```

```
./docker-compose.yml

services:
  simple-scraper-test:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app
    environment:
      # GitHub Actions env set
      - CI=true
      - GITHUB_ACTIONS=true
      - PW_CHROMIUM_USE_HEADLESS_SHELL=0
      - PLAYWRIGHT_CHROMIUM_USE_HEADLESS_SHELL=0
```

```
./dockerfile

# 파이썬 3.11 슬림 이미지를 기반으로 사용
FROM python:3.11-slim

# 작업 디렉토리를 /app으로 설정
WORKDIR /app

# Playwright와 웹 브라우저 실행에 필요한 시스템 종속성 설치
RUN apt-get update && apt-get install -y --no-install-recommends \
    # 필요한 브라우저 종속성
    libwoff1 \
    libharfbuzz-icu8 \
    libjpeg62-turbo-dev \
    libpng16-16 \
    libgdk-pixbuf-xlib-2.0-0 \
    libatk-bridge2.0-0 \
    libdbus-1-3 \
    libgtk-3-0 \
    libasound2 \
    libxkbcommon0 \
    libgbm1 \
    libnss3 \
    libxshmfence1 \
    fonts-noto-cjk \
    procps \
    curl \
    unzip \
    # apt 캐시 정리
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# 모든 프로젝트 파일을 컨테이너로 복사
COPY . /app

# pip 및 setuptools 업데이트
RUN python3 -m pip install --no-cache-dir -U pip setuptools

# 프로젝트 의존성 설치
RUN python3 -m pip install ".[dev]"

# Playwright에 맞는 브라우저를 다운로드
RUN python3 -m playwright install --with-deps chromium chrome

# 테스트 명령어 설정
CMD ["pytest", "-q"]
```

```
./.pytest_cache/CACHEDIR.TAG

Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html

```

```
./.pytest_cache/v/cache/nodeids

[
  "tests/test_intergration_site.py::test_attrs_mixed_href_filter",
  "tests/test_intergration_site.py::test_dynamic_href_added_later",
  "tests/test_intergration_site.py::test_dynamic_list_insert",
  "tests/test_intergration_site.py::test_dynamic_wait_visible_after_insert",
  "tests/test_intergration_site.py::test_empty_text_is_filtered_and_empty_href_ignored",
  "tests/test_intergration_site.py::test_encoding_utf8_paragraphs",
  "tests/test_intergration_site.py::test_index_list_extraction",
  "tests/test_intergration_site.py::test_links_href_extraction",
  "tests/test_intergration_site.py::test_longlist_count_and_edges",
  "tests/test_intergration_site.py::test_missing_selector_raises",
  "tests/test_intergration_site.py::test_nested_headlines_text_and_links",
  "tests/test_intergration_site.py::test_visibility_toggle",
  "tests/test_unit.py::test_model_to_dict",
  "tests/test_unit.py::test_utils_headers_and_uapool",
  "tests/test_unit.py::test_validation_errors"
]
```

```
./.pytest_cache/v/cache/lastfailed

{
  "tests/test_intergration_site.py::test_index_list_extraction": true,
  "tests/test_intergration_site.py::test_links_href_extraction": true,
  "tests/test_intergration_site.py::test_dynamic_wait_visible_after_insert": true,
  "tests/test_intergration_site.py::test_dynamic_list_insert": true,
  "tests/test_intergration_site.py::test_dynamic_href_added_later": true,
  "tests/test_intergration_site.py::test_attrs_mixed_href_filter": true,
  "tests/test_intergration_site.py::test_empty_text_is_filtered_and_empty_href_ignored": true,
  "tests/test_intergration_site.py::test_encoding_utf8_paragraphs": true,
  "tests/test_intergration_site.py::test_longlist_count_and_edges": true,
  "tests/test_intergration_site.py::test_nested_headlines_text_and_links": true,
  "tests/test_intergration_site.py::test_visibility_toggle": true,
  "tests/test_intergration_site.py::test_missing_selector_raises": true
}
```

